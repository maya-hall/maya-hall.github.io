<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Flood Risk Analysis</title>
   <link rel="stylesheet" href="../portfoliostyle.css" />
</head>
<body>

<header>
  <div class="container">
    <h1>Maya Hall</h1>
    <nav>
      <ul id="nav-links">
        <li><a href="../../index.html">About Me</a></li>
        <li><a href="../../index.html#projects" class="active">Projects</a></li>
        <li><a href="../../index.html#experience">Experience</a></li>
        <li><a href="../../index.html#education">Education</a></li>
        <li><a href="../../index.html#skills">Skills</a></li>
        <li><a href="../../index.html#cv">CV</a></li>
      </ul>
    </nav>
  </div>
</header>

<main>
  <div class="container">
  <div class="project-header">
    <h2 style = "font-size: 2rem;">Flood Risk Analysis</h2>
    <p style="font-size: 1.2rem;"><i>Assessing machine learning methods for flood risk identification in Buncombe County, North Carolina</i></p>
  </div>
    
  <div class="project-content">
    <p><strong>Overview and Methods:</strong> 
        This mini-project aimed to leverage different machine learning algorithms to predict flood risk areas in Buncombe County, NC. With more 
        frequent and severe storms and flood events, understanding the potential for flood risk is crucially important. We combined several machine 
        learning techniques to achieve our goal of predicting flood risk areas. As a feasibility study, we chose to examine conditions in Buncombe County 
        around September of 2024 due to the recent and tragic flood events. Based on existing literature, we selected the following variables to 
        include in our models: Topographic Wetness Index (TWI), distance to streams (DTS), Normalized Difference Vegetation Index (NDVI) from Landsat 9 OLI-2, 
        National Landcover Database (NLCD) data, and social vulnerabilty from the U.S. Census Bureau. For our flood risk layer, we used the National Flood 
        Hazard Layer (NFHL) 100-year floodplain data from FEMA, and decided to incorporate a 1 km buffer area as well. For modeling, we incorporated a 
        random forest classification, support vector machine classification, and conducted a principal component analysis. Our mini-project is still ongoing; 
        however, we have produced some preliminary results, which I am happy to share below!
         </p>


    <p><strong>Results:</strong> The initial random forest model performed well on "no flood" pixels (assigned as 0), but did not accurately classify 
        "yes flood" pixel presence (assigned as 1). The out-of-bag error for this first iteration was 2.25%, and the confusion matrix revealed that 
        very few "yes flood" pixels were properly classified.
    <img src="Flood_Risk_Maps_Unbalanced.jpg"/> </p>

    <p>Upon further investigation, the proportion of "no flood" pixels was much higher than "yes flood" pixels. Thus, we employed several dataset balancing 
        techniques to account for this imbalance. The first method involved bringing down the number of 0 pixels, so we randomly selected rows to reduce. 
        This resulted in an even number of 0 and 1 rows within our dataframe. This proved to significantly improve the random forest performance. 
        The second method that we attempted included assigning weights to the predictor variables. Pixels with a value of 1 were weighted more strongly 
        than pixels that contained a 0. This, however, did not yield very noticable improvements in model performance. We ultimately moved forward with 
        the reduction of randomly selected 0 pixels. The result of the balanced predicted flood risk can be seen below. 

    <img src="Flood_Risk_Maps_Balanced.jpg"/> </p>

    <p> <strong> Discussion and Conclusion:</strong> More results coming soon!</p>

    <p><strong>Tools Used:</strong> R, QGIS, GEE</p>

    <p><strong>Keywords:</strong> Remote Sensing, Carbon Loss, Tree Cover, Boreal Forest, Biomass</p>

    <p> <strong>Project Contributors:</strong> Katie Miller <i>(SVM, data acquisition, code refinement)</i>, Truman Anarella <i>(PCA, 
        data acquisition, code refinement)</i>, and Maya Hall <i>(random forest, data acquisition, code refinement)</i> </p>

    <a href="../../index.html#projects">&#8592; Back to Projects</a>
  </div>
  </div>
</main>

<footer>
  <p style="text-align:center; padding: 20px; color: #666">Contact: <a href="mailto:mayalhall@gmail.com">mayalhall@gmail.com</a> | 
     GitHub: <a href="https://github.com/maya-hall" target="_blank">maya-hall</a> | 
     LinkedIn: <a href="https://www.linkedin.com/in/mayalynnhall/" target="_blank">mayalynnhall</a></p>
</footer>

</body>
</html>
